{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5494fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from telegram_toolchain.data.database import get_conn\n",
    "from telegram_data_models import Message, Chat, MessageTextContent, Queue\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()   # loads .env from cwd (or parents)\n",
    "load_dotenv(\"../../credentials/credentials.env\")\n",
    "from sqlalchemy import select, func, case, create_engine\n",
    "from tqdm.auto import tqdm  # works in both notebooks & terminals\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "import glob\n",
    "%pip install duckdb\n",
    "import duckdb\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8217cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database setup\n",
    "db_user = os.environ.get(\"DB_USER\")\n",
    "db_pass = os.environ.get(\"DB_PASSWORD\")\n",
    "db_host = os.environ.get(\"DB_HOST\")\n",
    "db_port = os.environ.get(\"DB_PORT\")\n",
    "db_name = os.environ.get(\"DB_NAME\")\n",
    "\n",
    "db_url = f'postgresql+psycopg2://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Dask can't work with ORM models\n",
    "message_table = Message.__table__\n",
    "chat_table = Chat.__table__\n",
    "queue_table = Queue.__table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = f'postgresql+psycopg2://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Dask can't work with ORM models\n",
    "message_table = Message.__table__\n",
    "chat_table = Chat.__table__\n",
    "queue_table = Queue.__table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "    db_url,\n",
    "    pool_pre_ping=True,  # good for long streaming jobs\n",
    "    future=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lang = pd.read_parquet(\"../../data/chat_languages.parquet\")\n",
    "if df_lang.index.name == \"chat_id\" and \"chat_id\" not in df_lang.columns:\n",
    "    df_lang = df_lang.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ac123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- 1) Detailed edge SQL statement -------------------------\n",
    "\n",
    "# src_id: forward_from_chat_id if present else forward_from_id\n",
    "# sender_id: sender_chat_id if present else from_user_id\n",
    "# Type tags:\n",
    "#   src_is_chat = 1 iff forward_from_chat_id is not null\n",
    "#   sender_is_chat = 1 iff sender_chat_id is not null\n",
    "# dst is always a chat in your model (chat_id)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Build expressions once so we can reuse them consistently\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Source ID:\n",
    "# - Prefer forward_from_chat_id when present\n",
    "# - Fall back to forward_from_id otherwise\n",
    "# Some messages have *both* NULL → must be filtered out\n",
    "src_expr = func.coalesce(\n",
    "    message_table.c.forward_from_chat_id,\n",
    "    message_table.c.forward_from_id,\n",
    ")\n",
    "\n",
    "# Sender ID:\n",
    "# - Prefer sender_chat_id when present\n",
    "# - Fall back to from_user_id otherwise\n",
    "# Some messages have *both* NULL → must be filtered out\n",
    "sender_expr = func.coalesce(\n",
    "    message_table.c.sender_chat_id,\n",
    "    message_table.c.from_user_id,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Detailed edge list SQL\n",
    "#   One row per forwarded message\n",
    "#   Guaranteed non-null src and sender\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "stmt_detailed_edges = (\n",
    "    select(\n",
    "        message_table.c.id.label(\"msg_id\"),\n",
    "        # Source node (chat or user)\n",
    "        src_expr.label(\"src\"),\n",
    "        # Destination chat (always present)\n",
    "        message_table.c.chat_id.label(\"dst\"),\n",
    "        # Sender node (chat or user)\n",
    "        sender_expr.label(\"sender\"),\n",
    "        # Forward timestamp, truncated to second precision\n",
    "        func.date_trunc(\"second\", message_table.c.forward_date).label(\"ts\"),\n",
    "        # Type tag:\n",
    "        # 1 if src came from forward_from_chat_id (i.e., a chat)\n",
    "        # 0 if src came from forward_from_id (i.e., a user)\n",
    "        case(\n",
    "            (message_table.c.forward_from_chat_id.isnot(None), 1),\n",
    "            else_=0,\n",
    "        ).label(\"src_is_chat\"),\n",
    "        # Type tag:\n",
    "        # 1 if sender came from sender_chat_id (chat)\n",
    "        # 0 if sender came from from_user_id (user)\n",
    "        case(\n",
    "            (message_table.c.sender_chat_id.isnot(None), 1),\n",
    "            else_=0,\n",
    "        ).label(\"sender_is_chat\"),\n",
    "    )\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Filters\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Keep only forwarded messages\n",
    "    .where(message_table.c.forward_date.isnot(None))\n",
    "    # Ensure src is never NULL\n",
    "    # (required so pandas can safely cast to uint32)\n",
    "    .where(src_expr.isnot(None))\n",
    "    # Ensure sender is never NULL\n",
    "    # (required so pandas can safely cast to uint32)\n",
    "    .where(sender_expr.isnot(None))\n",
    ")\n",
    "\n",
    "# ------------------- 2) Lang map from df_lang -------------------------------\n",
    "\n",
    "df_lang = df_lang.copy()\n",
    "df_lang[\"chat_id\"] = df_lang[\"chat_id\"].astype(\"uint32\")\n",
    "lang_map = df_lang.set_index(\"chat_id\")[\"lang\"]  # chat_id -> lang (string/NA)\n",
    "\n",
    "# ------------------- 3) Parquet output schema -------------------------------\n",
    "\n",
    "edge_schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"msg_id\", pa.uint64()),\n",
    "        pa.field(\"src\", pa.uint32()),\n",
    "        pa.field(\"dst\", pa.uint32()),\n",
    "        pa.field(\"sender\", pa.uint32()),\n",
    "        pa.field(\"ts\", pa.timestamp(\"s\")),  # second precision\n",
    "        pa.field(\"src_is_chat\", pa.uint8()),  # 1 if src came from forward_from_chat_id else 0\n",
    "        pa.field(\"sender_is_chat\", pa.uint8()),  # 1 if sender came from sender_chat_id else 0\n",
    "        pa.field(\"primary\", pa.uint8()),  # 1 if (src_is_chat==1 and src in df_lang), else 0\n",
    "        pa.field(\"lang\", pa.string()),  # logic below\n",
    "    ]\n",
    ")\n",
    "\n",
    "out_dir = Path(\"../../data/timed_edge_list\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "last_id_path = out_dir / \"_last_id.txt\"\n",
    "last_id = int(last_id_path.read_text()) if last_id_path.exists() else 0\n",
    "print(f\"Resuming from msg_id > {last_id}\", flush=True)\n",
    "\n",
    "# Apply the resume filter\n",
    "stmt_run = stmt_detailed_edges.where(message_table.c.id > last_id).order_by(message_table.c.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366904f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1_000_000\n",
    "\n",
    "manifest_path = out_dir / \"_manifest.jsonl\"  # append-only checkpoint log\n",
    "\n",
    "\n",
    "def next_part_id():\n",
    "    existing = list(out_dir.glob(\"part-*.parquet\"))\n",
    "    if not existing:\n",
    "        return 1\n",
    "    last = max(int(p.stem.split(\"-\")[1]) for p in existing)\n",
    "    return last + 1\n",
    "\n",
    "\n",
    "part_id = next_part_id()\n",
    "print(f\"Resuming at part_id={part_id}\", flush=True)\n",
    "\n",
    "total_rows = 0\n",
    "t0 = time.time()  # job start time\n",
    "last_report = t0\n",
    "\n",
    "pbar = tqdm(unit=\"rows\", dynamic_ncols=True)\n",
    "\n",
    "total_bytes = sum(p.stat().st_size for p in out_dir.glob(\"part-*.parquet\"))\n",
    "size_gb = total_bytes / (1024**3)\n",
    "\n",
    "try:\n",
    "    with engine.connect().execution_options(stream_results=True) as conn:\n",
    "        for i, df_chunk in enumerate(pd.read_sql(stmt_run, conn, chunksize=chunksize), start=1):\n",
    "            # c0 = time.time()\n",
    "\n",
    "            df_chunk = df_chunk.astype(\n",
    "                {\n",
    "                    \"msg_id\": \"uint64\",\n",
    "                    \"src\": \"uint32\",\n",
    "                    \"dst\": \"uint32\",\n",
    "                    \"sender\": \"uint32\",\n",
    "                    \"src_is_chat\": \"uint8\",\n",
    "                    \"sender_is_chat\": \"uint8\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            df_chunk[\"ts\"] = pd.to_datetime(df_chunk[\"ts\"], errors=\"coerce\").dt.floor(\"s\")\n",
    "\n",
    "            src_lang = df_chunk[\"src\"].map(lang_map)\n",
    "            dst_lang = df_chunk[\"dst\"].map(lang_map)\n",
    "            primary = ((df_chunk[\"src_is_chat\"] == 1) & src_lang.notna()).astype(\"uint8\")\n",
    "\n",
    "            lang_col = pd.Series(pd.NA, index=df_chunk.index, dtype=\"string\")\n",
    "            same_lang = src_lang.eq(dst_lang) & src_lang.notna()\n",
    "            mask_primary_same = (primary == 1) & same_lang\n",
    "            lang_col[mask_primary_same] = src_lang[mask_primary_same].astype(\"string\")\n",
    "            mask_primary_other = (primary == 1) & ~mask_primary_same\n",
    "            lang_col[mask_primary_other] = \"NA\"\n",
    "            mask_not_primary = primary == 0\n",
    "            lang_col[mask_not_primary] = dst_lang[mask_not_primary].astype(\"string\")\n",
    "\n",
    "            df_chunk[\"primary\"] = primary\n",
    "            df_chunk[\"lang\"] = lang_col\n",
    "\n",
    "            table = pa.Table.from_pandas(df_chunk, schema=edge_schema, preserve_index=False)\n",
    "\n",
    "            # IMPORTANT: define paths per part\n",
    "            part_path = out_dir / f\"part-{part_id:06d}.parquet\"\n",
    "            tmp_path = out_dir / f\".part-{part_id:06d}.parquet.tmp\"\n",
    "\n",
    "            pq.write_table(table, tmp_path, compression=\"zstd\", use_dictionary=True)\n",
    "            os.replace(tmp_path, part_path)\n",
    "\n",
    "            # update size tracker\n",
    "            total_bytes += part_path.stat().st_size\n",
    "            size_gb = total_bytes / (1024**3)\n",
    "\n",
    "            # checkpoint after successful write\n",
    "            chunk_last_id = int(df_chunk[\"msg_id\"].max())\n",
    "            last_id_path.write_text(str(chunk_last_id))\n",
    "\n",
    "            # manifest\n",
    "            rec = {\n",
    "                \"part\": part_id,\n",
    "                \"file\": part_path.name,\n",
    "                \"rows\": int(table.num_rows),\n",
    "                \"written_at_unix\": time.time(),\n",
    "            }\n",
    "            with open(manifest_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "            part_id += 1\n",
    "\n",
    "            # progress\n",
    "            n = table.num_rows\n",
    "            total_rows += n\n",
    "            pbar.update(n)\n",
    "\n",
    "            now = time.time()\n",
    "            if now - last_report >= 30:\n",
    "                elapsed = now - t0  # correct: since job start\n",
    "                rps = total_rows / elapsed if elapsed > 0 else 0.0\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"parts\": part_id - 1,\n",
    "                        \"rows_M\": f\"{total_rows/1e6:.1f}\",\n",
    "                        \"rows/s\": f\"{rps:,.0f}\",\n",
    "                        \"dir_GB\": f\"{size_gb:.2f}\",\n",
    "                    }\n",
    "                )\n",
    "                last_report = now\n",
    "\n",
    "finally:\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a937a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"../../data/timed_edge_list\").resolve()\n",
    "parts_glob = str(out_dir / \"part-*.parquet\")\n",
    "\n",
    "# Write output in the parent directory of timed_edge_list\n",
    "final_path = (out_dir.parent / \"edges_sorted.parquet\").resolve()\n",
    "\n",
    "# Safety: ensure we are not writing inside the directory we plan to delete\n",
    "if out_dir == final_path or out_dir in final_path.parents:\n",
    "    raise RuntimeError(f\"Refusing: final_path={final_path} is inside out_dir={out_dir}\")\n",
    "\n",
    "# (Optional) temp dir for DuckDB spills (NOT inside out_dir if you're going to delete it)\n",
    "# temp_dir = str((out_dir.parent / \"_duckdb_tmp\").resolve())\n",
    "# Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"PRAGMA threads=8;\")  # tune to your CPU\n",
    "con.execute(\"PRAGMA preserve_insertion_order=false;\")\n",
    "# con.execute(f\"PRAGMA temp_directory='{temp_dir}';\")\n",
    "\n",
    "# --------- 1) Count rows in all part files ----------\n",
    "src_rows = con.execute(\n",
    "    f\"\"\"\n",
    "    SELECT COUNT(*)::BIGINT\n",
    "    FROM read_parquet('{parts_glob}')\n",
    "\"\"\"\n",
    ").fetchone()[0]\n",
    "\n",
    "if src_rows == 0:\n",
    "    raise RuntimeError(f\"No rows found in {parts_glob}. Aborting.\")\n",
    "\n",
    "print(f\"Found {src_rows:,} rows across partial parquet files.\")\n",
    "\n",
    "# --------- 2) Write sorted parquet (ALL columns) to parent dir ----------\n",
    "con.execute(\n",
    "    f\"\"\"\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM read_parquet('{parts_glob}')\n",
    "    ORDER BY src, sender, ts\n",
    ")\n",
    "TO '{str(final_path)}'\n",
    "(FORMAT PARQUET, COMPRESSION ZSTD);\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"Wrote sorted parquet: {final_path}\")\n",
    "\n",
    "# --------- 3) Verify row count in the output ----------\n",
    "out_rows = con.execute(\n",
    "    f\"\"\"\n",
    "    SELECT COUNT(*)::BIGINT\n",
    "    FROM read_parquet('{str(final_path)}')\n",
    "\"\"\"\n",
    ").fetchone()[0]\n",
    "\n",
    "print(f\"Output rows: {out_rows:,}\")\n",
    "\n",
    "if out_rows != src_rows:\n",
    "    raise RuntimeError(\n",
    "        f\"Row-count mismatch! parts={src_rows:,} output={out_rows:,}. \" f\"NOT deleting {out_dir}\"\n",
    "    )\n",
    "\n",
    "print(\"Row counts match ✅\")\n",
    "\n",
    "# --------- 4) Delete the entire timed_edge_list directory ----------\n",
    "# Extra safety: refuse to delete something too shallow like /, /home, etc.\n",
    "if len(out_dir.parts) < 3:\n",
    "    raise RuntimeError(f\"Refusing to delete suspiciously short path: {out_dir}\")\n",
    "\n",
    "shutil.rmtree(out_dir)\n",
    "print(f\"Deleted directory: {out_dir}\")\n",
    "\n",
    "print(\"Finalization complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-quality-control-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
