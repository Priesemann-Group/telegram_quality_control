{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select, create_engine, func\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "from datasketch import HyperLogLogPlusPlus\n",
    "\n",
    "from telegram_data_models import MessageTextContent, Message\n",
    "from telegram_quality_control.db import get_conn_string\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b887567",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = False\n",
    "\n",
    "if test_run:\n",
    "    chunk_size = 10_000\n",
    "else:\n",
    "    chunk_size = 100_000\n",
    "\n",
    "scratch_folder = Path(os.environ.get(\"SCRATCH_FOLDER\")) / \"duplicate_analysis\"\n",
    "scratch_folder.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Scratch folder: {scratch_folder}\")\n",
    "\n",
    "data_folder = Path(os.environ.get(\"OUTPUT_FOLDER\"))\n",
    "print(f\"Data folder: {data_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e6334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database setup\n",
    "db_url = get_conn_string(\".env\")\n",
    "\n",
    "content_table = MessageTextContent.__table__\n",
    "message_table = Message.__table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02aa693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(docs, hyperlog, length_distribution):\n",
    "    \"\"\"\n",
    "    Count the estimate of the number of unique docs and the length distribution.\n",
    "\n",
    "    Update the values of top_counter and hl\n",
    "    l.\n",
    "    \"\"\"\n",
    "    for line in docs:\n",
    "        if not line or len(line) == 0:\n",
    "            continue\n",
    "\n",
    "        # Update unique estimate\n",
    "        hyperlog.update(line.encode())\n",
    "\n",
    "        # Update length distribution\n",
    "        doc_length = len(line)\n",
    "        if doc_length not in length_distribution.keys():\n",
    "            length_distribution[doc_length] = 0\n",
    "        else:\n",
    "            length_distribution[doc_length] += 1\n",
    "\n",
    "    return hyperlog\n",
    "\n",
    "\n",
    "def download_chunk(db_conn_string, num_messages):\n",
    "    engine = create_engine(db_conn_string)\n",
    "\n",
    "    last_message_id = 0\n",
    "\n",
    "    while True:\n",
    "        with engine.connect() as conn:\n",
    "            sql = (\n",
    "                select(\n",
    "                    content_table.c.message_id,\n",
    "                    func.coalesce(content_table.c.text, content_table.c.caption).label('content'),\n",
    "                )\n",
    "                .where(content_table.c.message_id > last_message_id)\n",
    "                .order_by(content_table.c.message_id)\n",
    "                .limit(num_messages)\n",
    "            )\n",
    "\n",
    "            result = pd.read_sql_query(sql, conn, index_col=\"message_id\")\n",
    "\n",
    "            sql = (\n",
    "                select(message_table.c.id)\n",
    "                .where(message_table.c.id.in_(result.index.tolist()))\n",
    "                .where(message_table.c.forward_date.is_(None))\n",
    "            )\n",
    "\n",
    "            forward_dates = pd.read_sql_query(sql, conn, index_col=\"id\")\n",
    "            # keep only non-forwarded messages\n",
    "            result = result[result.index.isin(forward_dates.index)]\n",
    "\n",
    "        if result.empty:\n",
    "            break\n",
    "\n",
    "        last_message_id = int(result.index.max())\n",
    "\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f382188",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlog = HyperLogLogPlusPlus()\n",
    "length_distribution = {}\n",
    "\n",
    "message_stats = pd.read_sql_query(\"SELECT MAX(message_id) AS max FROM message_content\", db_url)\n",
    "\n",
    "max_message_id = int(message_stats[\"max\"].iloc[0])\n",
    "total_rows = 5505989600\n",
    "\n",
    "print(f\"Max message id: {max_message_id}\")\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "\n",
    "num_rows = 0\n",
    "average_download_time = []\n",
    "average_process_time = []\n",
    "\n",
    "generator = download_chunk(db_url, chunk_size)\n",
    "\n",
    "tic = datetime.now()\n",
    "\n",
    "for i, content_df in enumerate(generator):\n",
    "    tac = datetime.now()\n",
    "    num_rows += len(content_df)\n",
    "    process_chunk(content_df[\"content\"], hyperlog, length_distribution)\n",
    "    toc = datetime.now()\n",
    "\n",
    "    download_time = (tac - tic).total_seconds()\n",
    "    process_time = (toc - tac).total_seconds()\n",
    "\n",
    "    average_download_time.append(download_time)\n",
    "    average_process_time.append(process_time)\n",
    "\n",
    "    if test_run and i > 1000:\n",
    "        break\n",
    "    if i % 10 == 0:\n",
    "        current_id = content_df.index.max()\n",
    "        frac_ids = current_id / max_message_id * 100\n",
    "        frac_messages = num_rows / total_rows * 100\n",
    "        average_download_time = sum(average_download_time) / len(average_download_time)\n",
    "        average_process_time = sum(average_process_time) / len(average_process_time)\n",
    "        print(\n",
    "            f\"[{datetime.now().strftime('%H:%M:%S')}] chunk {i}, \\t{frac_ids:.4f}% of ids, \\t{frac_messages:.4f}% of messages, \\t{average_download_time:.2f} s. download, \\t{average_process_time:.2f} s. process\"\n",
    "        )\n",
    "        average_download_time = []\n",
    "        average_process_time = []\n",
    "\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        # checkpoint results\n",
    "        current_id = content_df.index.max()\n",
    "        print(f\"Checkpointing at message id {current_id}\")\n",
    "        with open(scratch_folder / f\"hyperlog_{current_id}.pkl\", 'wb') as f:\n",
    "            pickle.dump(hyperlog, f)\n",
    "\n",
    "        with open(scratch_folder / f\"length_distribution_{current_id}.pkl\", 'wb') as f:\n",
    "            pickle.dump(length_distribution, f)\n",
    "\n",
    "    tic = datetime.now()\n",
    "\n",
    "unique_estimate = hyperlog.count()\n",
    "fraction = unique_estimate / num_rows\n",
    "\n",
    "print(f\"Unique fraction: {fraction:.2f}\")\n",
    "\n",
    "with open(data_folder / \"unique_estimate.txt\", 'w') as f:\n",
    "    f.write(f\"unique_estimate: {unique_estimate}\\n\")\n",
    "    f.write(f\"total messages: {num_rows}\\n\")\n",
    "    f.write(f\"fraction: {fraction}\")\n",
    "\n",
    "length_dist_df = pd.DataFrame.from_dict(length_distribution, orient='index', columns=['count'])\n",
    "length_dist_df.index.name = 'length'\n",
    "length_dist_df = length_dist_df.sort_index()\n",
    "length_dist_df.to_csv(data_folder / \"message_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abe928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-quality-control-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
