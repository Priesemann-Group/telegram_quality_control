{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed77cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select, create_engine, func, text\n",
    "\n",
    "from telegram_data_models import Message, Chat, MessageTextContent, Entity\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from telegram_quality_control.db import get_conn_string\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = False\n",
    "\n",
    "if test_run:\n",
    "    chunk_size = 10_000  # number of entities to process in one chunk\n",
    "else:\n",
    "    chunk_size = 10_000\n",
    "\n",
    "entity_type = \"hashtag\"\n",
    "entity_type_name = \"MessageEntityType.HASHTAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df147975",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = get_conn_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_chunk(num_entities, db_conn_string):\n",
    "    engine = create_engine(db_conn_string)\n",
    "\n",
    "    last_entity_id = 0\n",
    "\n",
    "    while True:\n",
    "        with engine.connect() as conn:\n",
    "\n",
    "            sql = (\n",
    "                select(\n",
    "                    Entity.id,\n",
    "                    Entity.message_id,\n",
    "                    Entity.type,\n",
    "                    Entity.offset,\n",
    "                    Entity.length,\n",
    "                )\n",
    "                .where(Entity.type.in_([entity_type_name]))\n",
    "                .where(Entity.id > last_entity_id)\n",
    "                .order_by(Entity.id)\n",
    "                .limit(num_entities)\n",
    "            )\n",
    "\n",
    "            entities = pd.read_sql_query(sql, conn, index_col=\"id\")\n",
    "\n",
    "            sql = select(\n",
    "                MessageTextContent.message_id,\n",
    "                func.coalesce(MessageTextContent.text, MessageTextContent.caption).label('content'),\n",
    "            ).where(MessageTextContent.message_id.in_(entities[\"message_id\"].tolist()))\n",
    "\n",
    "            content = pd.read_sql_query(sql, conn, index_col=\"message_id\")\n",
    "\n",
    "            if entities.empty:\n",
    "                break\n",
    "\n",
    "            last_entity_id = int(entities.index.max())\n",
    "\n",
    "            yield entities, content\n",
    "\n",
    "\n",
    "def extract_utf16_substring(text, offset, length):\n",
    "    utf16_bytes = text.encode('utf-16-le')\n",
    "    # Each code unit is 2 bytes\n",
    "    start_byte = offset * 2\n",
    "    end_byte = (offset + length) * 2\n",
    "    substring_bytes = utf16_bytes[start_byte:end_byte]\n",
    "    return substring_bytes.decode('utf-16-le')\n",
    "\n",
    "\n",
    "def process_chunk(entities, content):\n",
    "    merged = entities.merge(content, left_on='message_id', right_index=True)\n",
    "    # Apply the extraction function\n",
    "    merged[entity_type] = merged.apply(\n",
    "        lambda row: extract_utf16_substring(row['content'], row['offset'], row['length']), axis=1\n",
    "    )\n",
    "    merged[\"entity_id\"] = merged.index\n",
    "    merged = merged.reset_index(drop=True)\n",
    "    merged = merged[['message_id', entity_type, 'entity_id']]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_run:\n",
    "    generator = download_chunk(chunk_size, db_url)\n",
    "    entities, content_df = next(generator)\n",
    "\n",
    "    print(content_df[\"content\"])\n",
    "\n",
    "    display(process_chunk(entities, content_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_run:\n",
    "    bmp_only = \"Hello ä½ å¥½ SUBSTRING\"\n",
    "    non_bmp_only = \"ð•³ð–Šð–‘ð–‘ð–” ð“—ð“®ð“µð“µð“¸ ðŸŒðŸš€ SUBSTRING\"\n",
    "    mixed = \"HelloðŸŒä¸–ç•Œð•³ð–Šð–‘ð–‘ð–” SUBSTRING\"\n",
    "\n",
    "    print(extract_utf16_substring(bmp_only, 9, 9))\n",
    "    print(extract_utf16_substring(non_bmp_only, 27, 9))\n",
    "    print(extract_utf16_substring(mixed, 20, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(db_url)\n",
    "\n",
    "table_name = f\"entity_{entity_type}s\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\n",
    "        text(\n",
    "            f'''\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            message_id BIGINT,\n",
    "            {entity_type} TEXT,\n",
    "            entity_id BIGINT\n",
    "        )\n",
    "    '''\n",
    "        )\n",
    "    )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_stats = pd.read_sql_query(\"SELECT MAX(message_id) AS max FROM entities\", db_url)\n",
    "\n",
    "max_message_id = int(entity_stats[\"max\"].iloc[0])\n",
    "\n",
    "print(f\"Max message id: {max_message_id}\")\n",
    "\n",
    "num_rows = 0\n",
    "average_download_time = []\n",
    "average_process_time = []\n",
    "\n",
    "generator = download_chunk(chunk_size, db_url)\n",
    "\n",
    "tic = datetime.now()\n",
    "\n",
    "for i, (entities, content) in enumerate(generator):\n",
    "    tac = datetime.now()\n",
    "\n",
    "    result = process_chunk(entities, content)\n",
    "    result.to_sql(table_name, db_url, if_exists='append', index=False)\n",
    "\n",
    "    toc = datetime.now()\n",
    "\n",
    "    download_time = (tac - tic).total_seconds()\n",
    "    process_time = (toc - tac).total_seconds()\n",
    "\n",
    "    average_download_time.append(download_time)\n",
    "    average_process_time.append(process_time)\n",
    "\n",
    "    if test_run and i > 1000:\n",
    "        break\n",
    "    if i % 10 == 0:\n",
    "        current_id = result[\"message_id\"].max()\n",
    "        frac_ids = current_id / max_message_id * 100\n",
    "        average_download_time = sum(average_download_time) / len(average_download_time)\n",
    "        average_process_time = sum(average_process_time) / len(average_process_time)\n",
    "        print(\n",
    "            f\"[{datetime.now().strftime('%H:%M:%S')}] chunk {i}, \\t{frac_ids:.4f}% of ids, \\t{average_download_time:.2f} s. download, \\t{average_process_time:.2f} s. process\"\n",
    "        )\n",
    "        average_download_time = []\n",
    "        average_process_time = []\n",
    "\n",
    "    tic = datetime.now()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-quality-control-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
