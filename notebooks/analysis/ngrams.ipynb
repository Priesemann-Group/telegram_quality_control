{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telegram_data_models import Message, MessageTextContent\n",
    "from telegram_quality_control.chat_language import ChatLanguage\n",
    "from telegram_quality_control.db import get_conn_string\n",
    "\n",
    "from datasketch import HyperLogLogPlusPlus\n",
    "from collections import Counter\n",
    "\n",
    "from itertools import batched\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sqlalchemy import select, create_engine, func\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb92794",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = False\n",
    "\n",
    "num_messages = 10_000  # number of messages to download in one go\n",
    "\n",
    "num_most_common = 1000\n",
    "\n",
    "max_counter_size = 50_000_000\n",
    "# max_counter_size = 100_000\n",
    "\n",
    "lang = \"en\"\n",
    "\n",
    "scratch_folder = Path(os.environ.get(\"SCRATCH_FOLDER\"))\n",
    "print(f\"Scratch folder: {scratch_folder}\")\n",
    "\n",
    "data_folder = Path(os.environ.get(\"OUTPUT_FOLDER\"))\n",
    "print(f\"Data folder: {data_folder}\")\n",
    "\n",
    "n_values = [1, 3, 10]  # n-grams to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95081f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database setup\n",
    "db_url = get_conn_string(\".env\")\n",
    "\n",
    "message_table = Message.__table__\n",
    "content_table = MessageTextContent.__table__\n",
    "language_table = ChatLanguage.__table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-load message ids\n",
    "message_id_path = scratch_folder / \"message_ids_english.csv\"\n",
    "\n",
    "if message_id_path.exists():\n",
    "    print(\"Loading message ids from cache\")\n",
    "\n",
    "    if test_run:\n",
    "        message_ids = pd.read_csv(\n",
    "            message_id_path, usecols=[\"id\"], dtype=int, nrows=num_messages * 100\n",
    "        )\n",
    "    else:\n",
    "        message_ids = pd.read_csv(message_id_path, usecols=[\"id\"], dtype=int)\n",
    "\n",
    "else:\n",
    "    print(\"Collecting the message ids\")\n",
    "    sql = (\n",
    "        select(\n",
    "            message_table.c.id,\n",
    "        )\n",
    "        .join(content_table, message_table.c.id == content_table.c.message_id)\n",
    "        # filter for non-empty text or caption\n",
    "        .where((content_table.c.text.isnot(None) | content_table.c.caption.isnot(None)))\n",
    "        .join(language_table, language_table.c.chat_id == message_table.c.chat_id)\n",
    "        .where(language_table.c.lang == lang)\n",
    "        .where(language_table.c.score > 0.8)\n",
    "    )\n",
    "\n",
    "    message_ids = pd.read_sql_query(sql, db_url)\n",
    "\n",
    "    message_ids.to_csv(message_id_path, index=False)\n",
    "\n",
    "message_ids = message_ids[\"id\"].tolist()\n",
    "message_ids.sort()\n",
    "print(message_ids[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ff1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cached files\n",
    "last_message_id = -1\n",
    "cache_file = None\n",
    "for file in scratch_folder.glob(f\"ngram_counters_*.pkl\"):\n",
    "    print(f\"Found cached file: {file}\")\n",
    "    last_id = int(file.stem.split(\"_\")[-1])\n",
    "    if last_id > last_message_id:\n",
    "        last_message_id = last_id\n",
    "        cache_file = file\n",
    "\n",
    "print(f\"Last processed message id: {last_message_id}\")\n",
    "if last_message_id != -1:\n",
    "    message_ids = [mid for mid in message_ids if mid > last_message_id]\n",
    "    print(f\"Remaining messages to process: {len(message_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bab173",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_id_chunks = list(batched(message_ids, n=num_messages))\n",
    "\n",
    "num_chunks = len(message_id_chunks)\n",
    "print(f\"Number of chunks: {num_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46138be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(content_df, ngram_counters):\n",
    "    \"\"\"\n",
    "    Count n-grams from messages and update the counters.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = content_df[\"content\"].tolist()\n",
    "\n",
    "    for message in messages:\n",
    "        words = message.strip().lower().split()\n",
    "        for n in ngram_counters.keys():\n",
    "            # Generate n-grams\n",
    "            if len(words) < n:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(words) - n + 1):\n",
    "                ngram = tuple(words[i : i + n])\n",
    "                ngram_counters[n][ngram] += 1\n",
    "\n",
    "    # Prune if the counters become too large\n",
    "    for n in n_values:\n",
    "        if len(ngram_counters[n]) > max_counter_size:\n",
    "            ngram_counters[n] = Counter(dict(ngram_counters[n].most_common(max_counter_size // 2)))\n",
    "\n",
    "    return ngram_counters\n",
    "\n",
    "\n",
    "def download_messages(db_conn_string, message_ids):\n",
    "    engine = create_engine(db_conn_string)\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        sql = select(\n",
    "            content_table.c.message_id,\n",
    "            func.coalesce(content_table.c.text, content_table.c.caption).label('content'),\n",
    "        ).where(content_table.c.message_id.in_(message_ids))\n",
    "\n",
    "        result = pd.read_sql_query(sql, conn, index_col=\"message_id\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a10e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cache_file is not None:\n",
    "    print(f\"Loading ngram counters from cache file: {cache_file}\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        ngram_counters = pickle.load(f)\n",
    "else:\n",
    "    print(\"Initializing new ngram counters\")\n",
    "    ngram_counters = {n: Counter() for n in n_values}\n",
    "\n",
    "total_messages = len(message_ids)\n",
    "\n",
    "num_rows = 0\n",
    "average_download_time = []\n",
    "average_process_time = []\n",
    "\n",
    "for i, message_ids in enumerate(message_id_chunks):\n",
    "    tic = datetime.now()\n",
    "    chunk = download_messages(db_url, message_ids)\n",
    "    tac = datetime.now()\n",
    "    ngram_counters = process_chunk(chunk, ngram_counters)\n",
    "    toe = datetime.now()\n",
    "    average_download_time.append((tac - tic).total_seconds())\n",
    "    average_process_time.append((toe - tac).total_seconds())\n",
    "\n",
    "    if test_run and i > 1000:\n",
    "        break\n",
    "    if i % 10 == 0:\n",
    "        frac = i * num_messages / total_messages * 100\n",
    "        average_download_time = sum(average_download_time) / len(average_download_time)\n",
    "        average_process_time = sum(average_process_time) / len(average_process_time)\n",
    "        print(\n",
    "            f\"[{datetime.now().strftime('%H:%M:%S')}] chunk {i}, \\t{frac:.4f}% messages \\t{average_download_time:.2f} s. download, \\t{average_process_time:.2f} s. process, \\t{len(ngram_counters[n_values[-1]])} 10-grams\"\n",
    "        )\n",
    "        average_download_time = []\n",
    "        average_process_time = []\n",
    "\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        # checkpoint results\n",
    "        current_id = int(max(message_ids))\n",
    "        print(f\"Checkpointing at message id {current_id}\")\n",
    "        with open(scratch_folder / f\"ngram_counters_{current_id}.pkl\", 'wb') as f:\n",
    "            pickle.dump(ngram_counters, f)\n",
    "\n",
    "        # delete previous checkpoints except for the second-to-last one\n",
    "        second_to_last_id = -1\n",
    "        for file in scratch_folder.glob(f\"ngram_counters_*.pkl\"):\n",
    "            last_id = int(file.stem.split(\"_\")[-1])\n",
    "            if last_id < current_id and last_id > second_to_last_id:\n",
    "                second_to_last_id = last_id\n",
    "\n",
    "        if second_to_last_id != -1:\n",
    "            for file in scratch_folder.glob(f\"ngram_counters_*.pkl\"):\n",
    "                last_id = int(file.stem.split(\"_\")[-1])\n",
    "                if last_id != current_id and last_id != second_to_last_id:\n",
    "                    print(f\"Deleting old checkpoint file: {file}\")\n",
    "                    file.unlink()\n",
    "\n",
    "most_common = {}\n",
    "for n in n_values:\n",
    "    # O(N log k) complexity where N = length of the counter, k = num_most_common\n",
    "    most_common[n] = ngram_counters[n].most_common(num_most_common)\n",
    "\n",
    "print(\"Calculated most common\")\n",
    "\n",
    "with open(data_folder / 'ngram_counters.pkl', 'wb') as f:\n",
    "    pickle.dump(ngram_counters, f)\n",
    "\n",
    "print(\"Pickle dump counters\")\n",
    "\n",
    "# Convert Counter objects to serializable format\n",
    "for n, counter in most_common.items():\n",
    "    json_counter = {}\n",
    "    for ngram, count in counter:\n",
    "        # Convert tuple n-grams to space-separated strings\n",
    "        if isinstance(ngram, tuple):\n",
    "            key = ' '.join(ngram)\n",
    "        else:\n",
    "            key = ngram\n",
    "        json_counter[key] = count\n",
    "\n",
    "    with open(data_folder / f'most_common_{n}-grams.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_counter, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-quality-control-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
