{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc6b0d0",
   "metadata": {},
   "source": [
    "# A notebook to parse URLs from messages in the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select, create_engine, Table, Column, Integer, ForeignKey, func, cast\n",
    "\n",
    "from telegram_data_models import Message, Chat, MessageTextContent, Entity\n",
    "from telegram_quality_control.chat_language import ChatLanguage\n",
    "\n",
    "from itertools import batched\n",
    "\n",
    "import multiprocessing as mp\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import LocalCluster\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from telegram_quality_control.urls import load_rating_resources, batch_rate_urls\n",
    "from telegram_quality_control.db import get_conn_string\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'en'\n",
    "lang_score = 0.8\n",
    "\n",
    "test_run = False\n",
    "\n",
    "num_workers = 1  # number of parallel workers to use\n",
    "if test_run:\n",
    "    chunk_size = 100  # number of entities to process in one chunk\n",
    "else:\n",
    "    chunk_size = 10_000\n",
    "\n",
    "scratch_folder = Path(os.environ.get(\"SCRATCH_FOLDER\")) / \"urls\"\n",
    "print(f\"Scratch folder: {scratch_folder}\")\n",
    "scratch_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_folder = Path(os.environ.get(\"OUTPUT_FOLDER\"))\n",
    "print(f\"Data folder: {data_folder}\")\n",
    "data_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb90a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = get_conn_string()\n",
    "\n",
    "chat_table = Chat.__table__\n",
    "message_table = Message.__table__\n",
    "content_table = MessageTextContent.__table__\n",
    "language_table = ChatLanguage.__table__\n",
    "entity_table = Entity.__table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef685ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-load entity ids\n",
    "entity_id_path = scratch_folder / \"entity_ids.csv\"\n",
    "\n",
    "if entity_id_path.exists():\n",
    "    print(\"Loading entity ids from cache\")\n",
    "\n",
    "    if test_run:\n",
    "        entity_ids = pd.read_csv(entity_id_path, usecols=[\"id\"], dtype=int, nrows=chunk_size * 100)\n",
    "    else:\n",
    "        entity_ids = pd.read_csv(entity_id_path, usecols=[\"id\"], dtype=int)\n",
    "\n",
    "else:\n",
    "    print(\"Collecting the entity ids\")\n",
    "    sql = (\n",
    "        select(\n",
    "            entity_table.c.id,\n",
    "            language_table.c.chat_id,\n",
    "        )\n",
    "        .join(message_table, message_table.c.id == entity_table.c.message_id)\n",
    "        .join(language_table, language_table.c.chat_id == message_table.c.chat_id)\n",
    "        .where(language_table.c.lang == lang)\n",
    "        .where(language_table.c.score > 0.8)\n",
    "    )\n",
    "\n",
    "    entity_ids = pd.read_sql_query(sql, db_url)\n",
    "\n",
    "    entity_ids.to_csv(scratch_folder / \"entity_ids.csv\")\n",
    "\n",
    "entity_ids = entity_ids[\"id\"].tolist()\n",
    "entity_ids.sort()\n",
    "print(entity_ids[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d8b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load already parsed entities\n",
    "if (scratch_folder / \"finished_entity_ids.parquet\").exists():\n",
    "    finished_entity_ids = pd.read_parquet(scratch_folder / \"finished_entity_ids.parquet\")[\n",
    "        \"entity_id\"\n",
    "    ].tolist()\n",
    "\n",
    "    print(f\"Already finished: {len(finished_entity_ids)} entities\")\n",
    "\n",
    "    entity_ids = list(set(entity_ids) - set(finished_entity_ids))\n",
    "\n",
    "    print(f\"Remaining: {len(entity_ids)} entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b072a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_chunks = list(batched(entity_ids, n=chunk_size))\n",
    "\n",
    "num_chunks = len(entity_id_chunks)\n",
    "print(f\"Number of chunks: {num_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52879915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(url_df, original_rating, updated_rating):\n",
    "    \"\"\"\n",
    "    Parse the URLs from all messages from the chats in `chat_ids`.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(url_df) == 0:\n",
    "        return None\n",
    "\n",
    "    result_original = batch_rate_urls(url_df, *original_rating, col=\"url\")\n",
    "\n",
    "    result_updated = batch_rate_urls(url_df, *updated_rating, col=\"url\")\n",
    "\n",
    "    matched_url_df = result_original.merge(\n",
    "        result_updated[[\"reliability\"]],\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_original\", \"_updated\"),\n",
    "    )\n",
    "\n",
    "    return matched_url_df\n",
    "\n",
    "\n",
    "def download_chunk(entity_ids, db_conn_string):\n",
    "    sql = (\n",
    "        select(\n",
    "            Message.id.label(\"message_id\"),\n",
    "            Entity.id,\n",
    "            Entity.type,\n",
    "            Entity.url,\n",
    "            func.substring(MessageTextContent.text, Entity.offset + 1, Entity.length).label(\n",
    "                'entity_text'\n",
    "            ),\n",
    "        )\n",
    "        .join(Entity, Entity.message_id == Message.id)\n",
    "        .join(MessageTextContent, MessageTextContent.message_id == Entity.message_id)\n",
    "        .where(Entity.type.in_([\"MessageEntityType.URL\", \"MessageEntityType.TEXT_LINK\"]))\n",
    "        .where(Entity.id.in_(entity_ids))\n",
    "    )\n",
    "\n",
    "    engine = create_engine(db_conn_string)\n",
    "    with engine.connect() as conn:\n",
    "        url_df = pd.read_sql_query(sql, conn, index_col=\"id\")\n",
    "\n",
    "    url_df[\"type\"] = url_df[\"type\"].replace(\n",
    "        {\"MessageEntityType.URL\": \"url\", \"MessageEntityType.TEXT_LINK\": \"text_link\"}\n",
    "    )\n",
    "\n",
    "    # The URL is stored in the 'url' column for TEXT_LINK and in the 'entity_text' column for URL\n",
    "    # This merges both columns.\n",
    "    url_df[\"url\"] = url_df[\"entity_text\"].where(url_df[\"type\"] == \"url\", url_df[\"url\"])\n",
    "    url_df = url_df.drop(columns=[\"entity_text\", \"type\"])\n",
    "\n",
    "    return url_df\n",
    "\n",
    "\n",
    "def append_to_parquet(df, path):\n",
    "    \"\"\"\n",
    "    Save the dataframe to a parquet file. If the file already exists, append the data.\n",
    "    \"\"\"\n",
    "    assert path.suffix == \".parquet\", \"Path must be a parquet file\"\n",
    "    if path.exists():\n",
    "        df.to_parquet(path, engine=\"fastparquet\", append=True)\n",
    "    else:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_parquet(path, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3114848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "original_rating = load_rating_resources(version=\"original\")\n",
    "updated_rating = load_rating_resources(version=\"updated\")\n",
    "\n",
    "cache_files = {\n",
    "    \"urls\": scratch_folder / \"urls.parquet\",\n",
    "    \"matched_urls\": scratch_folder / \"matched_urls.parquet\",\n",
    "    \"entity_ids\": scratch_folder / \"finished_entity_ids.parquet\",\n",
    "}\n",
    "\n",
    "# remove existing files\n",
    "# for file in cache_files.values():\n",
    "#     file.unlink(missing_ok=True)\n",
    "\n",
    "for i, entity_ids in enumerate(entity_id_chunks):\n",
    "    print(f\"[{datetime.now().strftime(\"%H:%M:%S\")}] chunk {i}/{num_chunks}\")\n",
    "    url_df = download_chunk(entity_ids, db_url)\n",
    "    matched_urls = process_chunk(url_df, original_rating, updated_rating)\n",
    "\n",
    "    if len(url_df) > 0:\n",
    "        append_to_parquet(url_df, cache_files[\"urls\"])\n",
    "        append_to_parquet(matched_urls, cache_files[\"matched_urls\"])\n",
    "\n",
    "    # save the entity_ids\n",
    "    append_to_parquet(pd.DataFrame(entity_ids, columns=[\"entity_id\"]), cache_files[\"entity_ids\"])\n",
    "\n",
    "\n",
    "for file in cache_files.values():\n",
    "    shutil.move(file, data_folder / file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78962c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-quality-control-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
